## SQLContext 编程

```scala
package com.imooc.spark

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext;

/**
  * SQLContext 使用
  */
object SQLContextApp {

  def main(args: Array[String]): Unit = {

    val path = "file:///home/jony/apps/packages/spark-2.1.0/python/test_support/sql/people.json"
    //创建相应的Context
    val sparkConf = new SparkConf()
    sparkConf.setAppName("SQLContextApp").setMaster("local[2]")

    val sc = new SparkContext(sparkConf)
    val sqlContext = new SQLContext(sc)

    //相关处理，处理JSON 文件
    val people = sqlContext.read.format("json").load(path)
    people.printSchema()
    people.show()

    //关闭资源
    sc.stop()
  }
}
```

## HiveContext 编程

```scala
package com.imooc.spark

import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object HiveContextApp {

  def main(args: Array[String]): Unit = {
    //创建相应的Context
    val sparkConf = new SparkConf()
//    sparkConf.setAppName("HiveContextApp").setMaster("local[2]")
    val sparkContext = new SparkContext()
    val hiveContext = new HiveContext(sparkContext)

    //相关处理
    hiveContext.table("emp").show()

    //关闭资源
    sparkContext.stop()
  }

}
```

## SparkSession 编程

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

object SparkSessionApp {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("SparkSessionApp").master("local[2]").getOrCreate()

    val people = spark.read.format("json").load("/home/jony/apps/packages/spark-2.1.0/dist/examples/src/main/resources/people.json")
    people.show()

    spark.stop()
  }
}
```

## Spark JDBC 

需要启动 thriftserver。

```scala
package com.imooc.spark

import java.sql.DriverManager

/**
  * deal with hvie data through jdbc
  */
object SparkSQLThriftServerApp {
    def main(args: Array[String]): Unit = {
        //get driver
        Class.forName("org.apache.hive.jdbc.HiveDriver")

        //get connection
        val connection = DriverManager.getConnection("jdbc:hive2://192.168.8.170:10000", "jony", "")

        //execute sql
        val pstmt = connection.prepareStatement("select empno, ename, sal from emp")
        val rs = pstmt.executeQuery()
        while (rs.next()) {
            println("empno:" + rs.getInt("empno") + ", ename:" + rs.getString("ename")
                    + ", sal:" + rs.getDouble("sal"))
        }

        //close connection
        rs.close()
        pstmt.close()
        connection.close()
    }
}
```

## DataFrame

DataFrame 始于其他语言：R、Python，后移植到 Spark 中。

DataFrame 是以列(列名，列类型，列值)形式构成的分布式数据集，按列赋予不同的名称。可以理解为一张表。

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

/**
  * DataFram API basic operations
  */
object DataFrameApp {
    def main(args: Array[String]): Unit = {
        //create spark session
        val sparkSession = SparkSession.builder().appName("DataFrameApp")
                .master("local[2]").getOrCreate()

        //load json file
        val peopleDF = sparkSession.read.format("json")
                .load("/home/jony/apps/packages/spark-2.1.0/dist/examples/src/main/resources/people.json")

        //print schema info from dataframe
        peopleDF.printSchema()

        //print 20 contents from dataframe
        peopleDF.show()

        //print name column data
        peopleDF.select("name").show()
        peopleDF.select(peopleDF.col("name"), (peopleDF.col("age") + 10).as("add_age")).show()

        //filter age
        peopleDF.filter(peopleDF.col("age") > 19).show()

        //group by some column and do aggregation
        peopleDF.groupBy("age").count().show()

        //close sparksession
        sparkSession.stop()
    }
}
```

使用 DateFrame 操作 RDD 数据：

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

/**
  * DataFrame RDD interoperate
  */
object DataFrameRDDApp {
    def main(args: Array[String]): Unit = {
        //get sparksession
        val spark = SparkSession.builder()
                .appName("DataFrameRDDApp")
                .master("local[2]")
                .getOrCreate()

        //RDD => DataFrame
        val rdd = spark.sparkContext.textFile("/home/jony/resource/infos.txt")

        //需要导入隐式转换
        import spark.implicits._
        val infoDF = rdd.map(_.split(","))
                .map(line => Info(line(0).toInt, line(1), line(2).toInt))
                .toDF()
        infoDF.show()

        //采用DF API 的操作获取数据
        infoDF.filter(infoDF.col("age") > 30).show()

        //采用sql语句的方式获取结果
        infoDF.createOrReplaceTempView("infos")
        spark.sql("select * from infos where age > 30").show()

        spark.stop()

    }

    case class Info(id: Int, name: String, age: Int)
}
```

DataFrame 常用 API 操作：

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

/**
  * DataFrame 其他操作
  */
object DataFrameCase {
    def main(args: Array[String]): Unit = {
        val spark = SparkSession.builder().appName("DataFrameCase").master("local[2]").getOrCreate()
        val rdd = spark.sparkContext.textFile("/home/jony/resource/data/student.data")

        import spark.implicits._
        val studentDF = rdd.map(_.split("\\|")).map(
            line => Student(
                line(0).toInt,
                line(1),
                line(2),
                line(3))).toDF()

//        studentDF.show(30, false)
//        studentDF.take(10).foreach(println)
//        studentDF.head(3).foreach(println)
//        studentDF.select("email", "name").show(30, false)
//        studentDF.filter("name=='' OR name == 'NULL'").show(30, false)

        //找出以名字以m开头的人
//        studentDF.filter("substr(name, 0, 1) = 'M'").show()

        //sort
//        studentDF.sort("name", "id").show
//        studentDF.sort(studentDF("name"), studentDF("id").desc).show

//        studentDF.select(studentDF("name").as("Student_name")).show()

        val studentDF2 = rdd.map(_.split("\\|")).map(
            line => Student(
                line(0).toInt,
                line(1),
                line(2),
                line(3))).toDF()

        //join
        studentDF.join(studentDF2, studentDF.col("id") === studentDF2.col("id")).show()

        spark.stop()
    }

    case class Student(id: Int, name: String, phone: String, email: String)
}
```

## DateSet 操作

DateFrame 与 DateSet 转换：

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

/**
  * DateSet operation
  */
object DateSetApp {
    def main(args: Array[String]): Unit = {
        val spark = SparkSession.builder().appName("DateSetApp").master("local[2]").getOrCreate()

        val df = spark.read.option("header", "true").option("inferSchema", "true").csv("/home/jony/resource/data/sales.csv").toDF

        df.show()

        //需要导入隐式转换
        import spark.implicits._
        val ds = df.as[Sales]
        ds.map(line => line.itemId).show()

        spark.stop()
    }

    case class Sales(transactionId: Int, customerId: Int, itemId: Int, amountPaid: Double)
}
```

静态类型和运行时类型安全检查

|  | SQL | DataFrame | DataSet |
|:--:|:--:|:--------:|:-------:|
| Syntax Errors| Runtime | CompileTime | CompileTime |
| Analysis Errors | Runtime | Runtime | CompileTime |

使用 DataSet 可以更早地发现语法和字段错误，方便提前发现问题。