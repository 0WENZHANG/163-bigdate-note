## 下载 Hive

Hive1.1.0 下载地址：[hive-1.1.0-cdh5.7.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz)

## 安装

1. 加压配置环境变量

    ```conf
    # Hive
    export HIVE_HOME=/home/wst/apps/hive-1.1.0-cdh5.7.0
    export PATH=$HIVE_HOME/bin:$PATH
    ```

2. 配置 hive-env.sh

    ```conf
    HADOOP_HOME=/home/wst/apps/hadoop-2.6.0-cdh5.7.0
    ```

3. 配置 hive-site.xml

```xml
<configuration>
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>1</value>
      <description>password to use against metastore database</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://192.168.8.169:3306/hive_sparksql?createDatabaseIfNotExist=true&amp;useSSL=true</value>
      <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>Username to use against metastore database</description>
    </property>
</configuration>
```

4. 启动 hive


## 使用 hive

1. hive官方建表语句：

  ```sql
  CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)
    [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]
    [COMMENT table_comment]
    [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
    [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
    [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]
      ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
      [STORED AS DIRECTORIES]
    [
    [ROW FORMAT row_format] 
    [STORED AS file_format]
      | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
    ]
    [LOCATION hdfs_path]
    [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)
    [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)
  
  CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
    LIKE existing_table_or_view_name
    [LOCATION hdfs_path];
  
  data_type
    : primitive_type
    | array_type
    | map_type
    | struct_type
    | union_type  -- (Note: Available in Hive 0.7.0 and later)
  
  primitive_type
    : TINYINT
    | SMALLINT
    | INT
    | BIGINT
    | BOOLEAN
    | FLOAT
    | DOUBLE
    | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)
    | STRING
    | BINARY      -- (Note: Available in Hive 0.8.0 and later)
    | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)
    | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)
    | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)
    | DATE        -- (Note: Available in Hive 0.12.0 and later)
    | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)
    | CHAR        -- (Note: Available in Hive 0.13.0 and later)
  
  array_type
    : ARRAY < data_type >
  
  map_type
    : MAP < primitive_type, data_type >
  
  struct_type
    : STRUCT < col_name : data_type [COMMENT col_comment], ...>
  
  union_type
    : UNIONTYPE < data_type, data_type, ... >  -- (Note: Available in Hive 0.7.0 and later)
  
  row_format
    : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
          [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
          [NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)
    | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
  
  file_format:
    : SEQUENCEFILE
    | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)
    | RCFILE      -- (Note: Available in Hive 0.6.0 and later)
    | ORC         -- (Note: Available in Hive 0.11.0 and later)
    | PARQUET     -- (Note: Available in Hive 0.13.0 and later)
    | AVRO        -- (Note: Available in Hive 0.14.0 and later)
    | JSONFILE    -- (Note: Available in Hive 4.0.0 and later)
    | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname
  
  constraint_specification:
    : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]
      [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE 
  ```

2. 基本建表语句

  ```sql
  create table hive_wordcount()
  ```

3. 将如下内容放入hive表中：

  ```
  ➜  data cat emp.txt
  7369	SMITH	CLERK	7902	1980-12-17	800.00		20
  7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30
  7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30
  7566	JONES	MANAGER	7839	1981-4-2	2975.00		20
  7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30
  7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30
  7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10
  7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20
  7839	KING	PRESIDENT		1981-11-17	5000.00		10
  7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30
  7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20
  7900	JAMES	CLERK	7698	1981-12-3	950.00		30
  7902	FORD	ANALYST	7566	1981-12-3	3000.00		20
  7934	MILLER	CLERK	7782	1982-1-23	1300.00		10
  8888	HIVE	PROGRAM	7839	1988-1-23	10300.00    	
  ```

  ```
  ➜  data cat dept.txt 
  10      ACCOUNTING      NEW YORK
  20      RESEARCH        DALLAS
  30      SALES   CHICAGO
  40      OPERATIONS      BOSTON
  ```

4. 创建 emp 表和 dept 表：

```sql
create table emp(
  empno int,
  ename string,
  job string,
  mgr int,
  hiredate string,
  sal double,
  comm double,
  deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```

```sql
create table dept(
  deptno int,
  dname string,
  location string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```

5. 加载数据到 emp 表和 dept 表：

```sql
load data local inpath '/home/wst/apps/packages/data/emp.txt' into table emp;
load data local inpath '/home/wst/apps/packages/data/dept.txt' into table dept;

load data local inpath '/mnt/home/1015146591/jars/data/emp.txt' into table emp;
load data local inpath '/mnt/home/1015146591/jars/data/dept.txt' into table dept;

load data local inpath '/root/resource/data/emp.txt' into table emp;
load data local inpath '/root/resource/data/dept.txt' into table dept;

```

## 使用 spark-sql 访问 hive 数据

1. 将 hive-site.xml 放入 sqprk 的配置文件中.

2. 将 mysql 驱动程序放入 spark 的 jars 目录中.

3. 启动 spark-shell

    ```
    spark-shell --master "local[2]"
    ```

    访问 hive 中的数据:

    ```
    scala> spark.sql("show tables").show
    +--------+---------+-----------+
    |database|tableName|isTemporary|
    +--------+---------+-----------+
    | default|     dept|      false|
    | default|      emp|      false|
    +--------+---------+-----------+
    ```

    ```
    scala> spark.sql("select * from emp e join dept d on e.deptno=d.deptno").show
    +-----+------+---------+----+----------+------+------+------+------+----------+--------+
    |empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|deptno|     dname|location|
    +-----+------+---------+----+----------+------+------+------+------+----------+--------+
    | 7369| SMITH|    CLERK|7902|1980-12-17| 800.0|  null|    20|    20|  RESEARCH|  DALLAS|
    | 7499| ALLEN| SALESMAN|7698| 1981-2-20|1600.0| 300.0|    30|    30|     SALES| CHICAGO|
    | 7521|  WARD| SALESMAN|7698| 1981-2-22|1250.0| 500.0|    30|    30|     SALES| CHICAGO|
    | 7566| JONES|  MANAGER|7839|  1981-4-2|2975.0|  null|    20|    20|  RESEARCH|  DALLAS|
    | 7654|MARTIN| SALESMAN|7698| 1981-9-28|1250.0|1400.0|    30|    30|     SALES| CHICAGO|
    | 7698| BLAKE|  MANAGER|7839|  1981-5-1|2850.0|  null|    30|    30|     SALES| CHICAGO|
    | 7782| CLARK|  MANAGER|7839|  1981-6-9|2450.0|  null|    10|    10|ACCOUNTING|NEW YORK|
    | 7788| SCOTT|  ANALYST|7566| 1987-4-19|3000.0|  null|    20|    20|  RESEARCH|  DALLAS|
    | 7839|  KING|PRESIDENT|null|1981-11-17|5000.0|  null|    10|    10|ACCOUNTING|NEW YORK|
    | 7844|TURNER| SALESMAN|7698|  1981-9-8|1500.0|   0.0|    30|    30|     SALES| CHICAGO|
    | 7876| ADAMS|    CLERK|7788| 1987-5-23|1100.0|  null|    20|    20|  RESEARCH|  DALLAS|
    | 7900| JAMES|    CLERK|7698| 1981-12-3| 950.0|  null|    30|    30|     SALES| CHICAGO|
    | 7902|  FORD|  ANALYST|7566| 1981-12-3|3000.0|  null|    20|    20|  RESEARCH|  DALLAS|
    | 7934|MILLER|    CLERK|7782| 1982-1-23|1300.0|  null|    10|    10|ACCOUNTING|NEW YORK|
    +-----+------+---------+----+----------+------+------+------+------+----------+--------+
    ```

  spark 查询的速度比 hive 快很多,因为 spark 是基于内存操作的.

4. 启动 spark-sql 查询数据

    ```
    spark-sql --master "local[2]"
    ```

    然后就可以使用 sql 查询数据了

5. 查看执行计划

    ```sql
    create table t(
        key string,
        value string
    );

    explain extended select a.key*(2+3), b.value from t a join t b on a.key = b.key and a.key > 3;
    ```

    执行计划:

    ```
    == Parsed Logical Plan ==
    'Project [unresolvedalias(('a.key * (2 + 3)), None), 'b.value]
    +- 'Join Inner, (('a.key = 'b.key) && ('a.key > 3))
    :- 'UnresolvedRelation `t`, a
    +- 'UnresolvedRelation `t`, b

    == Analyzed Logical Plan ==
    (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE)): double, value: string
    Project [(cast(key#80 as double) * cast((2 + 3) as double)) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#84, value#83]
    +- Join Inner, ((key#80 = key#82) && (cast(key#80 as double) > cast(3 as double)))
    :- SubqueryAlias a
    :  +- MetastoreRelation default, t
    +- SubqueryAlias b
        +- MetastoreRelation default, t

    == Optimized Logical Plan ==
    Project [(cast(key#80 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#84, value#83]
    +- Join Inner, (key#80 = key#82)
    :- Project [key#80]
    :  +- Filter (isnotnull(key#80) && (cast(key#80 as double) > 3.0))
    :     +- MetastoreRelation default, t
    +- Filter (isnotnull(key#82) && (cast(key#82 as double) > 3.0))
        +- MetastoreRelation default, t

    == Physical Plan ==
    *Project [(cast(key#80 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#84, value#83]
    +- *SortMergeJoin [key#80], [key#82], Inner
    :- *Sort [key#80 ASC NULLS FIRST], false, 0
    :  +- Exchange hashpartitioning(key#80, 200)
    :     +- *Filter (isnotnull(key#80) && (cast(key#80 as double) > 3.0))
    :        +- HiveTableScan [key#80], MetastoreRelation default, t
    +- *Sort [key#82 ASC NULLS FIRST], false, 0
        +- Exchange hashpartitioning(key#82, 200)
            +- *Filter (isnotnull(key#82) && (cast(key#82 as double) > 3.0))
                +- HiveTableScan [key#82, value#83], MetastoreRelation default, t
    Time taken: 0.432 seconds, Fetched 1 row(s)
    19/04/29 21:44:21 INFO CliDriver: Time taken: 0.432 seconds, Fetched 1 row(s)
    ```

## thriftserver 模式

1. 启动 thriftserver

    ```
    ./start-thriftserver.sh --master "local[2]" --hiveconf hive.server2.thrift.port=14000
    ```

2. 使用 beeline 连接 thriftserver

    ```
    beeline -u jdbc:hive2://192.168.60.21:10000 -n root
    ```

3. thriftserver 模式与 spark-shell 和 spark-sql 的区别:可以开多个窗口,