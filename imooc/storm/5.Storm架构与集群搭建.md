## Storm 架构

Storm 架构可以类比为公司中人员关系，如下图所示。

![1](https://github.com/jiaoqiyuan/pics/raw/master/sxt/Storm-structure.png)

- Storm 的架构类似于 Hadoop，是主从模型(Master/Slave)。

- 其中 Numbus 是主节点，负责任务(Task)的调度和资源分配。

- Supervisor 是从节点，是负责执行任务的具体部分，启动和停止自己管理的 Worker 进程。

    Supervisor 中有具体执行 task 的 Worker。其中 task 对应的就是 Spout 和 Bolt。

    Supervisor 可以启动多个 Worker， 具体启动多少个可以配置。
    
    一个 Topology 可以运行在多个 Worker 上，也可以通过配置来指定。

- Storm 中各个节点是无状态的，节点上的信息(元数据)会存储到 Zookeeper 中。

- Worker 是运行具体组件逻辑(Spout/Bolt)的进程。

以下都是线程，以上都是进程。

- Task 是 Spout 和 Bolt 线程的统称。

- executor：Spout 和 Bolt 可能会共享一个线程。

- Storm 调优的关键是设置好一个 Topology 启动多少个 Worker， 一个Worker上启动多少个 executor， 然后一个 executor 启动多少个 Task。

## Storm 安装

### 单机版 Storm

1. 下载并解压 Storm1.2.2 ：[点击这里](https://www-us.apache.org/dist/storm/apache-storm-1.2.2/apache-storm-1.2.2.tar.gz)。

    ```
    [root@node01 ~]# wget https://www-us.apache.org/dist/storm/apache-storm-1.2.2/apache-storm-1.2.2.tar.gz

    [root@node01 ~]# tar xzvf ~/apache-storm-1.2.2.tar.gztar.gz -C /opt/sxt/
    ```

2. 添加 Storm 到系统环境变量:

    ```
    # Storm
    export STORM_HOME=/opt/sxt/storm-1.2.2
    export PATH=$STORM_HOME/bin:$PATH
    ```

    ```
    [root@node01 storm-1.2.2]# source /etc/profile
    ```

3. 配置 Strom

    配置 JAVA_HOME :

    ```
    export JAVA_HOME=/opt/sxt/jdk1.8.0_201
    ```

4. 启动 Storm ：

    Storm 部分参数配置：

        dev-zookeeper 启动 ZK
        numbus 启动主节点
        supervisor：启动从节点
        ui：启动界面
        logviewer：启动日志查看服务。

    前台启动 zookeeper ：

    ```
    storm dev-zookeeper
    ```

    在 bin/ 目录下后台启动 `zookeeper`：

    ```
    [root@node01 bin]# nohup sh storm dev-zookeeper &
    ```

    在 bin/ 目录下后台启动 `nimbus`：

    ```
    [root@node01 bin]# nohup sh storm nimbus &
    ```

    启动 storm 的 `UI`：

    ```
    [root@node01 bin]# nohup sh storm ui &
    ```

    在浏览器输入 `http://node01:8080` 查看 Storm UI.

    后台启动 `supervisor`：

    ```
    [root@node01 bin]# nohup sh storm supervisor &
    [root@node01 bin]# jps
    1824 dev_zookeeper
    2114 core
    1927 nimbus
    2346 Jps
    2252 Supervisor
    ```

    后台启动 `logviewer`：

    ```
    [root@node01 bin]# nohup sh storm logviewer &
    [root@node01 bin]# jps
    1824 dev_zookeeper
    2496 Jps
    2114 core
    1927 nimbus
    2252 Supervisor
    2398 logviewer
    ```

5. 提交作业到集群运行：

    先打包程序并上传到集群上。

    使用 strom 运行 jar：

    ```
    [root@node01 bin]# storm jar /root/jars/storm-1.0-SNAPSHOT.jar com.imooc.bigdata.ClusterSumStormTopology
    ```

    在网页端 `http://node01:8000` 可以查看相信信息。

6. Storm 其他命令的使用

    查看当前正在跑的 Topology：

    ```
    storm list
    ```

    停止作业，可以通过 UI 的方式：

    ```
    Topology Summary -> ClusterSumStormTopology -> Topology actions -> kill
    ```

    通过命令行停止作业：

    ```
    storm kill ClusterSumStormTopology
    ```

    通过 kill 杀死其他 storm 进程：


    ```
    [root@node01 bin]# jps
    1824 dev_zookeeper
    2114 core
    1927 nimbus
    2252 Supervisor
    3038 Jps
    2398 logviewer
    [root@node01 bin]# kill -9 1824 2114 1927 2252 2398
    [root@node01 bin]# jps
    3048 Jps
    [1]   Killed                  nohup sh storm dev-zookeeper
    [2]   Killed                  nohup sh storm nimbus
    [3]   Killed                  nohup sh storm ui
    [4]-  Killed                  nohup sh storm supervisor
    [5]+  Killed                  nohup sh storm logviewer
    [root@node01 bin]# jps
    3058 Jps
    [root@node01 bin]# 
    ```

## Storm 集群搭建

1. 部署规划：

    |        | NN-1 | NN-2 | DN | ZK | ZKFC | JNN | RS | NM | Nimbus(ui) | Supervisor(logviewer) | 
    |:------:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
    | node01 | * |   |   |   | * | * |   |   | * | * |
    | node02 |   | * | * | * | * | * |   | * |   | * |
    | node03 |   |   | * | * |   | * | * | * |   | * |
    | node04 |   |   | * | * |   |   | * | * |   |   |

2. 拷贝 strom 文件夹到 node02/node03 上：

    ```
    [root@node01 sxt]# scp -r storm-1.2.2/ root@node02:`pwd`
    [root@node01 sxt]# scp -r storm-1.2.2/ root@node03:`pwd`
    ```

3. 配置 node02/node03 的环境变量：

    ```
    # Storm
    export STORM_HOME=/opt/sxt/storm-1.2.2
    export PATH=$STORM_HOME/bin:$PATH
    ```

    ```
    [root@node02 sxt]# source /etc/profile
    [root@node03 sxt]# source /etc/profile
    ```

4. 修改 Storm `storm.yaml` 配置文件：

    ```
    storm.zookeeper.servers:
        - "node02"
        - "node03"

    storm.local.dir: "/var/sxt/storm"

    supervisor.slots.ports:
        - 6700
        - 6701
        - 6702
        - 6703

    nimbus.seeds: ["node01"]
    ```

5. 拷贝配置文件到 node02/node03：

    ```
    [root@node01 conf]# scp storm.yaml root@node02:`pwd`
    [root@node01 conf]# scp storm.yaml root@node03:`pwd`
    ```

6. 启动

    启动 Nimbus：

    ```
    [root@node01 bin]# nohup sh storm nimbus &
    [root@node01 bin]# jps
    1682 nimbus
    1817 Jps
    ```

    启动 ui：

    ```
    [root@node01 bin]# nohup sh storm ui &
    [root@node01 bin]# jps
    1682 nimbus
    1965 Jps
    1869 core
    ```

    在 node01/node02/node03 上启动 supervisor 和 logviewer：

    ```
    [root@node01 bin]# nohup sh storm supervisor &
    [root@node01 bin]# nohup sh storm logviewer &
    [root@node01 bin]# jps
    1682 nimbus
    2134 logviewer
    1997 Supervisor
    1869 core
    2255 Jps

    [root@node02 bin]# nohup sh storm supervisor &
    [root@node02 bin]# nohup sh storm logviewer &
    [root@node02 bin]# jps
    1698 QuorumPeerMain
    2067 Jps
    1845 Supervisor
    1979 logviewer

    [root@node03 bin]# nohup sh storm supervisor &
    [root@node03 bin]# nohup sh storm logviewer &
    [root@node03 bin]# jps
    1777 Jps
    1554 Supervisor
    1668 logviewer
    1288 QuorumPeerMain
    ```

7. 提交作业到分布式 Storm 集群运行

    ```
    [root@node01 bin]# storm jar /root/jars/storm-1.0-SNAPSHOT.jar com.imooc.bigdata.ClusterSumStormTopology
    ```

## Storm 并行度问题

在集群规模一定的情况下，任务量增大后需要想办法提高 Storm 的数据处理效率，这就引出了 Storm 的并行度问题，在集群可承载范围内增加 Storm 并行度可以提高整体数据处理效率。

在 Spout 向 Bolt 发送数据的时候使用多线程的方式即可.

![2](https://github.com/jiaoqiyuan/pics/raw/master/sxt/relationships-worker-processes-executors-tasks.png)

一个 Topology 可以运行在一个或多个 Worker 上，即一个 Worker 可能只运行一个 Topology 的一部分，可以通过代码调整，但是一个 Worker 只能跑一个 Topology，即 Topology 和 Worker 是一对多的关系。

Worker 是独立的进程，运行在 Supervisor 里面，默认情况下一个 Supervisor 启动四个 Worker。Worker 内部启动的是 Executor 线程，一个 Worker 进程可以启动多个 Executor 线程。

一个 Executor 里可以运行一个或多个 Task，Spout 和 Bolt 就是运行在 Task 中的。Task 的数量 >= Executor 的数量。

## 并行度调整

先关闭 acker， 方便查看 worker/executor/task 数量。

- worker 数量调整

    在向集群提交作业前，通过配置配置信息手动设置worker的数量即可实现 worker 数量调整。


    ```java
    Config config = new Config();
    config.setNumWorkers(2);
    config.setNumAckers(0);
    StormSubmitter.submitTopology(name, config, builder.createTopology());
    ```

    执行后通过 UI 界面可以看到有两个 Worker.

- executor 数量调整

    在使用 `setSpout/setBolt` 设置 Spout 和 Bolt 时通过指定第三个参数来设置 executor 的数量。

    ```java
    //根据Spout和Bolt构建出TopologyBuilder，Storm中任何作业都是通过Topology提交的，Topology中需要指定Spout和Bolt的顺序
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("DataSourceSpout", new DataSourceSpout(), 2);
    builder.setBolt("SumBolt", new SumBolt(), 2).shuffleGrouping("DataSourceSpout");
    ```

    执行后在 UI 界面查看到有 2 个 Worker， 4个 executor 4 个 Task。

- Task 数量调整

    通过 `setNumTasks` 来设置 Task 的数量。

    ```java
    //根据Spout和Bolt构建出TopologyBuilder，Storm中任何作业都是通过Topology提交的，Topology中需要指定Spout和Bolt的顺序
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("DataSourceSpout", new DataSourceSpout(), 2).setNumTasks(4);
    builder.setBolt("SumBolt", new SumBolt(), 2).setNumTasks(4).shuffleGrouping("DataSourceSpout");
    ```

    执行后在 UI 界面上可以看到有 2 个 worker， 4 个 Executor， 8 个 Task。

打开 acker，去掉 `config.setNumAckers(0);`。观察 worker/executor/task 数量，重新打包执行后通过 UI 可以看到有 2 个 worker， 6 个 executor，10 个 task，这是因为 2 个 worker 对应需要 2 个 acker，也就是两个 executor 线程，默认启动两个 acker task。Spout 和 Bolt 都是设置为 4 个 Task，在加上 2 个 acker 的 task，就是 10 个 task。

- 并行度案例分析

    ![2](https://github.com/jiaoqiyuan/pics/raw/master/sxt/example-of-a-running-topology.png)

    该 Topology 由三个组件构成： BlueSpout/GreenBolt/YelloBolt，BlueSpout 发送数据到 GreenBolt，GreenSpout再将数据输出到 YelloBolt。代码如下：

    ```java
    Config conf = new Config();
    conf.setNumWorkers(2); // use two worker processes

    topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

    topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
                .setNumTasks(4)
                .shuffleGrouping("blue-spout");

    topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
                .shuffleGrouping("green-bolt");

    StormSubmitter.submitTopology(
            "mytopology",
            conf,
            topologyBuilder.createTopology()
        );
    ```

    这里设置使用 2 个 worker ，BlueSpout 设置使用 2 个 Executor，GreenBolt 设置使用 2 个 Executor 4 个 task ，YellowBolt 设置使用 6 个 executor.

    提交运行后，一共有 2 个 worker 进程，并行度也就是线程数量一共有 2 + 2 + 6 = 10 个线程，每个 woker 进程均摊这 10 个线程，也就是每个 Worker 进程运行 5 个线程。需要注意的是 GreenBolt 设置使用 2 个 Executor 4 个 task，因此每个 executor 线程中运行 2 个 task。

- 如何更改正在运行的拓扑的并行性

    1. 通过 WebUI 界面调整。

    1. 通过客户端命令：

        `mytopology` 用于设置 worker 数量。

        `blue-spout/yellow-bolt` 是具体 Spout 的名称，可以设置具体 executor 数量

        ```
        ## Reconfigure the topology "mytopology" to use 5 worker processes,
        ## the spout "blue-spout" to use 3 executors and
        ## the bolt "yellow-bolt" to use 10 executors.

        $ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10
        ```

## Stream Grouping

- Shuffle grouping

- Fields grouping

- Partial Key grouping

- All grouping

- Global grouping

- None grouping

- Direct grouping

- Local or shuffle grouping

## RPC 

[RPC](https://baike.baidu.com/item/%E8%BF%9C%E7%A8%8B%E8%BF%87%E7%A8%8B%E8%B0%83%E7%94%A8%E5%8D%8F%E8%AE%AE/6893245?fromtitle=RPC&fromid=609861) - 远程过程调用(Remote Procedure Call)。

RPC 调用：[知乎链接](https://www.zhihu.com/question/25536695)。

RPC 性能好坏的关键是序列化的速度。

### 基于 Hadoop 的 RPC 实现




