## DataSet API 开发概述

> DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.

Flink 中的 DataSet 程序是实现数据集转换（例如，过滤，映射，连接，分组）的常规程序。 最初从某些源创建数据集（例如，通过读取文件或从本地集合创建）。 结果通过接收器返回，接收器可以例如将数据写入（分布式）文件或标准输出（例如命令行终端）。 Flink 程序可以在各种环境中运行，独立运行或嵌入其他程序中。 执行可以在本地 JVM 中执行，也可以在许多计算机的集群上执行。

## DataSource

数据源创建原始数据集，通常是基于 [InputFormat](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java) 来创建的。Flink 的 `ExecutionEnvironment` 有一些使用内嵌的格式来进行数据处理的方法。

基于文件的（生产环境中常用）：

- readTextFile(path) / TextInputFormat - 按行读取文件并将其作为字符串返回。

- readTextFileWithValue(path) / TextValueInputFormat - 按行读取文件并将其作为StringValues返回。 StringValues是可变字符串。

- readCsvFile（path）/ CsvInputFormat - 解析逗号（或其他字符）分隔字段的文件。返回元组，案例类对象或POJO的DataSet。支持基本的java类型及其Value对应的字段类型。

- readFileOfPrimitives（path，delimiter）/ PrimitiveInputFormat - 使用给定的分隔符解析新行（或其他char序列）分隔的原始数据类型（如String或Integer）的文件。

- readSequenceFile（Key，Value，path）/ SequenceFileInputFormat - 创建JobConf并从类型为SequenceFileInputFormat，Key class和Value类的指定路径中读取文件，并将它们作为Tuple2 <Key，Value>返回。

基于集合的（测试和学习中常用）：

- fromCollection（Seq） - 从Seq创建数据集。集合中的所有元素必须属于同一类型。

- fromCollection（Iterator） - 从迭代器创建数据集。该类指定迭代器返回的元素的数据类型。

- fromElements（elements：_ *） - 根据给定的对象序列创建数据集。所有对象必须属于同一类型。

- fromParallelCollection（SplittableIterator） - 并行地从迭代器创建数据集。该类指定迭代器返回的元素的数据类型。

- generateSequence（from，to） - 并行生成给定时间间隔内的数字序列。

### DataSet API 开发使用

[项目地址](./flink-train-scala/src/main/scala)

## [Transformation](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/#dataset-transformations)

转换/算子（Map, FlatMap, MapPartition, Filter, Reduce, ReduceGroup, Aggregate, Distinct, Join, OuterJoin, CoGroup, Cross, Union, Rebalance, Hash-Partition, Custon Partition, Sort Partition, First-n）

- Map

    Scala:

    ```scala
    def mapFunction(env: ExecutionEnvironment): Unit = {
        import org.apache.flink.api.scala._
        val data = env.fromCollection(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
        data.print()
        println("--------------")
        //data.map((x: Int) => x * 2).print()
        //data.map((x) => x * 2).print()
        //data.map(x => x * 2).print()
        data.map(_*2).print()
    }
    ```

    Java:

    ```java
    public static void mapFunction(ExecutionEnvironment env) throws Exception {
        List<Integer> list = new ArrayList<Integer>();
        for (int i = 0; i <= 10; i++) {
            list.add(i);
        }
        DataSource<Integer> dataSource = env.fromCollection(list);
        dataSource.map(new MapFunction<Integer, Integer>() {
            @Override
            public Integer map(Integer value) throws Exception {
                return value + 1;
            }
        }).print();
    }
    ```

- filter

    scala:

    ```scala
    def filterFunction(env: ExecutionEnvironment): Unit = {
        import org.apache.flink.api.scala._
        //链式编程
        env.fromCollection(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
            .map(_ + 1)
            .filter(_ > 5)
            .print()
    }
    ```

    java:

    ```java
    public static void fileterFunction(ExecutionEnvironment env) throws Exception {
        List<Integer> list = new ArrayList<Integer>();
        for (int i = 0; i <= 10; i++) {
            list.add(i);
        }

        DataSource<Integer> dataSource = env.fromCollection(list);
        dataSource.map(new MapFunction<Integer, Integer>() {
            @Override
            public Integer map(Integer value) throws Exception {
                return value + 1;
            }
        }).filter(new FilterFunction<Integer>() {
            @Override
            public boolean filter(Integer value) throws Exception {
                return value > 5;
            }
        }).print();
    }
    ```

- mapPartition

    scala:

    ```scala
    def mapPartitionFunction(env: ExecutionEnvironment): Unit = {
        val students = new ListBuffer[String]
        for (i <- 1 to 100) {
            students.append("student: " + i)
        }

        import org.apache.flink.api.scala._
        val data = env.fromCollection(students).setParallelism(4)
    //        data.map(x => {
    //            //获取Connection
    //            val connection = DBUtils.getConnnection()
    //            println(connection + "...")
    //
    //            //保存数据到DB
    //            DBUtils.returnConnection(connection)
    //        }).print()
        data.mapPartition(x => {
            val connection = DBUtils.getConnnection()
            println(connection + "....")
            DBUtils.returnConnection(connection)
            x
        }).print()
    }
    ```

    java:

    ```java
    public static void mapPartitionFunction(ExecutionEnvironment env) throws Exception {
        List<String> list = new ArrayList<String>();
        for (int i = 0; i <= 100; i++) {
            list.add("student" + i);
        }

        DataSource<String> dataSource = env.fromCollection(list).setParallelism(6);
    //        dataSource.map(new MapFunction<String, String>() {
    //            @Override
    //            public String map(String input) throws Exception {
    //                String connection = DBUtils.getConnnection();
    //                System.out.println("connection = " + connection);
    //                DBUtils.returnConnection(connection);
    //
    //                return input;
    //            }
    //        }).print();
        dataSource.mapPartition(new MapPartitionFunction<String, String>() {
            @Override
            public void mapPartition(Iterable<String> inputs, Collector<String> collector) throws Exception {
                String connection = DBUtils.getConnnection();
                System.out.println("connection = " + connection);
                DBUtils.returnConnection(connection);

            }
        }).print();
    }
    ```

- first

    scala:

    ```scala
    def firstFunction(env:ExecutionEnvironment): Unit ={
        val info = ListBuffer[(Int, String)]()
        info.append((1, "hadoop"))
        info.append((1, "spark"))
        info.append((1, "flink"))
        info.append((2, "java"))
        info.append((2, "Spring"))
        info.append((3, "Linux"))
        info.append((4, "Vue"))

        import org.apache.flink.api.scala._
        val data = env.fromCollection(info)
        //data.first(4).print()
        //println("============")
        //data.groupBy(0).first(2).print()
        data.groupBy(0).sortGroup(1, Order.ASCENDING).first(2).print()
    }
    ```

    java:

    ```java
    public static void firstFunction(ExecutionEnvironment env) throws Exception {
        List<Tuple2<Integer, String>> info = new ArrayList<Tuple2<Integer, String>>();
        info.add(new Tuple2<>(1, "hadoop"));
        info.add(new Tuple2<>(1, "spark"));
        info.add(new Tuple2<>(1, "flink"));
        info.add(new Tuple2<>(2, "java"));
        info.add(new Tuple2<>(2, "Spring"));
        info.add(new Tuple2<>(3, "Linux"));
        info.add(new Tuple2<>(4, "Vue"));

        DataSource<Tuple2<Integer, String>> data = env.fromCollection(info);
        data.first(3).print();
        System.out.println("======================");
        data.groupBy(0).first(2).print();
        System.out.println("======================");
        data.groupBy(0).sortGroup(1, Order.DESCENDING).first(2).print();

    }
    ```
    
- flatMap

    scala:

    ```scala
    def flatMapFunction(env: ExecutionEnvironment): Unit = {
        val info = ListBuffer[String]()
        info.append("hadoop,spark")
        info.append("hadoop,flink")
        info.append("flink,flink")
        info.append("flink,flink")
        import org.apache.flink.api.scala._
        val data = env.fromCollection(info)
        //data.map(_.split(",")).print()
        //data.flatMap(_.split(",")).print()
        data.flatMap(_.split(",")).map((_, 1)).groupBy(0).sum(1).print()
    }
    ```

    java:

    ```java
    public static void flatMapFunction(ExecutionEnvironment env) throws Exception {
        List<String> info = new ArrayList<>();
        info.add("hadoop,spark");
        info.add("hadoop, flink");
        info.add("flink,flink");
        DataSource<String> data = env.fromCollection(info);
        data.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String value, Collector<String> collector) throws Exception {
                String[] splits = value.split(",");
                for (String split : splits) {
                    collector.collect(split.trim());
                }
            }
        }).map(new MapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(String value) throws Exception {
                return new Tuple2<>(value, 1);
            }
        }).groupBy(0).sum(1).print();
    }
    ```

- distinct

    scala:

    ```scala
    def distinctFunction(env: ExecutionEnvironment): Unit = {
        val info = ListBuffer[String]()
        info.append("hadoop,spark")
        info.append("hadoop,flink")
        info.append("flink,flink")
        info.append("flink,flink")

        import org.apache.flink.api.scala._
        val data = env.fromCollection(info)
        data.flatMap(_.split(",")).distinct().print()
    }
    ```

    java:

    ```java
    public static void distinctFunction(ExecutionEnvironment env) throws Exception {
        List<String> info = new ArrayList<>();
        info.add("hadoop,spark");
        info.add("hadoop, flink");
        info.add("flink,flink");
        DataSource<String> data = env.fromCollection(info);
        data.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String value, Collector<String> collector) throws Exception {
                String[] splits = value.split(",");
                for (String split : splits) {
                    collector.collect(split.trim());
                }
            }
        }).distinct().print();
    }
    ```

- join

    scala:

    ```scala
    def joinFunction(env: ExecutionEnvironment): Unit = {
        val info1 = ListBuffer[(Int, String)]()
        info1.append((1, "hadoop"))
        info1.append((2, "spark"))
        info1.append((3, "flink"))
        info1.append((4, "java"))
        info1.append((5, "Spring"))
        info1.append((6, "Linux"))
        info1.append((7, "Vue"))

        val info2 = ListBuffer[(Int, String)]()
        info2.append((1, "beijing"))
        info2.append((2, "shanghai"))
        info2.append((3, "chengdu"))
        info2.append((4, "guangzhou"))
        info2.append((5, "hangzhou"))
        info2.append((6, "zhengzhou"))
        info2.append((7, "shenzhen"))

        import org.apache.flink.api.scala._
        val data1 = env.fromCollection(info1)
        val data2 = env.fromCollection(info2)

        data1.join(data2).where(0).equalTo(0).apply((first, second) =>{
            (first._1, first._2, second._2)
        }).print()
    }
    ```

    java:

    ```java
    public static void joinFunction(ExecutionEnvironment env) throws Exception {
        List<Tuple2<Integer, String>> info1 = new ArrayList<Tuple2<Integer, String>>();
        info1.add(new Tuple2<>(1, "hadoop"));
        info1.add(new Tuple2<>(2, "spark"));
        info1.add(new Tuple2<>(3, "flink"));
        info1.add(new Tuple2<>(4, "java"));
        info1.add(new Tuple2<>(5, "Spring"));
        info1.add(new Tuple2<>(6, "Linux"));

        List<Tuple2<Integer, String>> info2 = new ArrayList<Tuple2<Integer, String>>();
        info2.add(new Tuple2<>(1, "hadoop"));
        info2.add(new Tuple2<>(2, "spark"));
        info2.add(new Tuple2<>(3, "flink"));
        info2.add(new Tuple2<>(4, "java"));
        info2.add(new Tuple2<>(5, "Spring"));
        info2.add(new Tuple2<>(7, "Vue"));

        DataSource<Tuple2<Integer, String>> data1 = env.fromCollection(info1);
        DataSource<Tuple2<Integer, String>> data2 = env.fromCollection(info2);
        
        data1.join(data2).where(0).equalTo(0).with(new JoinFunction<Tuple2<Integer, String>, Tuple2<Integer, String>, Tuple3<Integer, String, String>>() {
            @Override
            public Tuple3<Integer, String, String> join(Tuple2<Integer, String> first, Tuple2<Integer, String> second) throws Exception {
                return new Tuple3<>(first.f0, first.f1, second.f1);
            }
        }).print();
    }
    ```

- OuterJoin

    scala:

    ```scala
    def outerJoinFunction(env: ExecutionEnvironment): Unit ={
        val info1 = ListBuffer[(Int, String)]()
        info1.append((1, "hadoop"))
        info1.append((2, "spark"))
        info1.append((3, "flink"))
        info1.append((4, "java"))
        info1.append((5, "Spring"))
        info1.append((6, "Linux"))

        val info2 = ListBuffer[(Int, String)]()
        info2.append((1, "beijing"))
        info2.append((2, "shanghai"))
        info2.append((3, "chengdu"))
        info2.append((4, "guangzhou"))
        info2.append((5, "hangzhou"))
        info2.append((7, "shenzhen"))

        import org.apache.flink.api.scala._
        val data1 = env.fromCollection(info1)
        val data2 = env.fromCollection(info2)

        data1.leftOuterJoin(data2).where(0).equalTo(0).apply((first, second) =>{
            if (second == null) {
                (first._1, first._2, "-")
            } else {
                (first._1, first._2, second._2)
            }
        }).print()

        println("==========================")

        data1.rightOuterJoin(data2).where(0).equalTo(0).apply((first, second) =>{
            if (first == null) {
                (second._1, "-", second._2)
            } else {
                (first._1, first._2, second._2)
            }
        }).print()

        println("===========================")

        data1.fullOuterJoin(data2).where(0).equalTo(0).apply((first, second) =>{
            if (first == null) {
                (second._1, "-", second._2)
            } else if (second == null){
                (first._1, first._2, "-")
            } else {
                (first._1, first._2, second._2)
            }
        }).print()
    }
    ```

    java

    ```java
   private static void outerJoinFunction(ExecutionEnvironment env) throws Exception {
        List<Tuple2<Integer, String>> info1 = new ArrayList<Tuple2<Integer, String>>();
        info1.add(new Tuple2<>(1, "hadoop"));
        info1.add(new Tuple2<>(2, "spark"));
        info1.add(new Tuple2<>(3, "flink"));
        info1.add(new Tuple2<>(4, "java"));
        info1.add(new Tuple2<>(5, "Spring"));
        info1.add(new Tuple2<>(6, "Linux"));

        List<Tuple2<Integer, String>> info2 = new ArrayList<Tuple2<Integer, String>>();
        info2.add(new Tuple2<>(1, "hadoop"));
        info2.add(new Tuple2<>(2, "spark"));
        info2.add(new Tuple2<>(3, "flink"));
        info2.add(new Tuple2<>(4, "java"));
        info2.add(new Tuple2<>(5, "Spring"));
        info2.add(new Tuple2<>(7, "Vue"));

        DataSource<Tuple2<Integer, String>> data1 = env.fromCollection(info1);
        DataSource<Tuple2<Integer, String>> data2 = env.fromCollection(info2);

        data1.leftOuterJoin(data2).where(0).equalTo(0).with(new JoinFunction<Tuple2<Integer, String>, Tuple2<Integer, String>, Tuple3<Integer, String, String>>() {
            @Override
            public Tuple3<Integer, String, String> join(Tuple2<Integer, String> first, Tuple2<Integer, String> second) throws Exception {
                if (second == null) {
                    return new Tuple3<>(first.f0, first.f1, "-");
                } else {
                    return new Tuple3<>(first.f0, first.f1, second.f1);
                }
            }
        }).print();

        System.out.println("==============================");

        data1.rightOuterJoin(data2).where(0).equalTo(0).with(new JoinFunction<Tuple2<Integer, String>, Tuple2<Integer, String>, Tuple3<Integer, String, String>>() {
            @Override
            public Tuple3<Integer, String, String> join(Tuple2<Integer, String> first, Tuple2<Integer, String> second) throws Exception {
                if (first == null) {
                    return new Tuple3<>(second.f0, "-", second.f1);
                } else {
                    return new Tuple3<>(first.f0, first.f1, second.f1);
                }
            }
        }).print();

        System.out.println("===============================");

        data1.fullOuterJoin(data2).where(0).equalTo(0).with(new JoinFunction<Tuple2<Integer, String>, Tuple2<Integer, String>, Tuple3<Integer, String, String>>() {
            @Override
            public Tuple3<Integer, String, String> join(Tuple2<Integer, String> first, Tuple2<Integer, String> second) throws Exception {
                if (first == null) {
                    return new Tuple3<>(second.f0, "-", second.f1);
                } else if (second == null) {
                    return new Tuple3<>(first.f0, first.f1, "-");
                } else {
                    return new Tuple3<>(first.f0, first.f1, second.f1);
                }
            }
        }).print();
    }
    ```



## Sink

## 计数器

## 分布式缓存

## 