## DataSet API 开发概述

> DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.

Flink 中的 DataSet 程序是实现数据集转换（例如，过滤，映射，连接，分组）的常规程序。 最初从某些源创建数据集（例如，通过读取文件或从本地集合创建）。 结果通过接收器返回，接收器可以例如将数据写入（分布式）文件或标准输出（例如命令行终端）。 Flink 程序可以在各种环境中运行，独立运行或嵌入其他程序中。 执行可以在本地 JVM 中执行，也可以在许多计算机的集群上执行。

## DataSource

数据源创建原始数据集，通常是基于 [InputFormat](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java) 来创建的。Flink 的 `ExecutionEnvironment` 有一些使用内嵌的格式来进行数据处理的方法。

基于文件的（生产环境中常用）：

- readTextFile(path) / TextInputFormat - 按行读取文件并将其作为字符串返回。

- readTextFileWithValue(path) / TextValueInputFormat - 按行读取文件并将其作为StringValues返回。 StringValues是可变字符串。

- readCsvFile（path）/ CsvInputFormat - 解析逗号（或其他字符）分隔字段的文件。返回元组，案例类对象或POJO的DataSet。支持基本的java类型及其Value对应的字段类型。

- readFileOfPrimitives（path，delimiter）/ PrimitiveInputFormat - 使用给定的分隔符解析新行（或其他char序列）分隔的原始数据类型（如String或Integer）的文件。

- readSequenceFile（Key，Value，path）/ SequenceFileInputFormat - 创建JobConf并从类型为SequenceFileInputFormat，Key class和Value类的指定路径中读取文件，并将它们作为Tuple2 <Key，Value>返回。

基于集合的（测试和学习中常用）：

- fromCollection（Seq） - 从Seq创建数据集。集合中的所有元素必须属于同一类型。

- fromCollection（Iterator） - 从迭代器创建数据集。该类指定迭代器返回的元素的数据类型。

- fromElements（elements：_ *） - 根据给定的对象序列创建数据集。所有对象必须属于同一类型。

- fromParallelCollection（SplittableIterator） - 并行地从迭代器创建数据集。该类指定迭代器返回的元素的数据类型。

- generateSequence（from，to） - 并行生成给定时间间隔内的数字序列。

### DataSet API 开发使用

[项目地址](./flink-train-scala/src/main/scala)

## [Transformation](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/#dataset-transformations)

转换/算子（Map, FlatMap, MapPartition, Filter, Reduce, ReduceGroup, Aggregate, Distinct, Join, OuterJoin, CoGroup, Cross, Union, Rebalance, Hash-Partition, Custon Partition, Sort Partition, First-n）

- Map

    Scala:

    ```scala
    def mapFunction(env: ExecutionEnvironment): Unit = {
        import org.apache.flink.api.scala._
        val data = env.fromCollection(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
        data.print()
        println("--------------")
        //data.map((x: Int) => x * 2).print()
        //data.map((x) => x * 2).print()
        //data.map(x => x * 2).print()
        data.map(_*2).print()
    }
    ```

    Java:

    ```java
    public static void mapFunction(ExecutionEnvironment env) throws Exception {
        List<Integer> list = new ArrayList<Integer>();
        for (int i = 0; i <= 10; i++) {
            list.add(i);
        }
        DataSource<Integer> dataSource = env.fromCollection(list);
        dataSource.map(new MapFunction<Integer, Integer>() {
            @Override
            public Integer map(Integer value) throws Exception {
                return value + 1;
            }
        }).print();
    }
    ```

- 

## Sink

## 计数器

## 分布式缓存

## 