## 大数据处理流程

MapReduce：

    input -> map/reduce -> output

Storm:

    input -> Spout/Bolt -> output

Spark:

    input -> transformation/action -> output

[Flink:](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/api_concepts.html)

    input -> transformation/sink -> output

处理过程类似，都是从源头获取数据，进行处理后输出，只不过处理的方式方法不同。 

## [DataSet 和 DataStream](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/api_concepts.html#dataset-and-datastream)

DataSet 用于处理批数据（有界的数据），DateStream 用于处理流数据（无界的数据）。

注意 DataSet 和 DataStream 是不可变的（immutable）。

## [Flink 编程模型](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/api_concepts.html#anatomy-of-a-flink-program)

1. Obtain an execution environment, 获取执行环境

1. Load/create the initial data, 获取数据

1. Specify transformations on this data, 转换数据

1. Specify where to put the results of your computations, 指定输出位置

1. Trigger the program execution 触发执行（批处理不需要）

具体参考 [Anatomy of a Flink Program](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/api_concepts.html#anatomy-of-a-flink-program).

## 延迟执行 Lazy Evaluation

Flink 是懒加载，只有在调用 execute 方法时，才会执行操作。


## 指定 Keys

一些转换操作比如 join，coGroup，keyBy，groupBy 需要在元素集合上定义 key，其他操作比如 Reduce，GroupReduce，Aggregate，Windows 等操作允许数据在使用之前基于 key 进行分组。

DateSet：

```java
DataSet<...> input = // [...]
DataSet<...> reduced = input
  .groupBy(/*define key here*/)
  .reduceGroup(/*do something*/);
```

DataStream:

```java
DataStream<...> input = // [...]
DataStream<...> windowed = input
  .keyBy(/*define key here*/)
  .window(/*window specification*/);
```

### 指定 Tuples Keys

指定单个 Tuple 作为 key：

```java
DataStream<Tuple3<Integer,String,Long>> input = // [...]
KeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0)
```

也可以指定多个 key，第一个和第二个键组成复合键：

```java
DataStream<Tuple3<Integer,String,Long>> input = // [...]
KeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0,1)
```

### 使用Field Expressions定义键

可以使用嵌套字段比如，直接使用 `word` 字段进行分组：

```java
// some ordinary POJO (Plain old Java Object)
public class WC {
  public String word;
  public int count;
}
DataStream<WC> words = // [...]
DataStream<WC> wordCounts = words.keyBy("word").window(/*window specification*/);
```

再比如：

```java
public static class WC {
  public ComplexNestedClass complex; //nested POJO
  private int count;
  // getter / setter for private field (count)
  public int getCount() {
    return count;
  }
  public void setCount(int c) {
    this.count = c;
  }
}
public static class ComplexNestedClass {
  public Integer someNumber;
  public float someFloat;
  public Tuple3<Long, Long, String> word;
  public IntWritable hadoopCitizen;
}
```

使用如下分组时：

- "count": The count field in the WC class.

- "complex": Recursively selects all fields of the field complex of POJO type ComplexNestedClass.

- "complex.word.f2": Selects the last field of the nested Tuple3.

- "complex.hadoopCitizen": Selects the Hadoop IntWritable type.

### 使用 key 选择函数定义 key

可以通过 KeySelector 自定义 key：

```java
// some ordinary POJO
public class WC {public String word; public int count;}
DataStream<WC> words = // [...]
KeyedStream<WC> keyed = words
  .keyBy(new KeySelector<WC, String>() {
     public String getKey(WC wc) { return wc.word; }
   });
```

### 指定 Transformation 函数

自己实现一个转换函数：

```java
class MyMapFunction implements MapFunction<String, Integer> {
  public Integer map(String value) { return Integer.parseInt(value); }
};
data.map(new MyMapFunction());
```

使用匿名内部类：

```java
data.map(new MapFunction<String, Integer> () {
  public Integer map(String value) { return Integer.parseInt(value); }
});
```

使用 Java8 的 Lambda 表达式：

```java
data.filter(s -> s.startsWith("http://"));
data.reduce((i1,i2) -> i1 + i2);
```