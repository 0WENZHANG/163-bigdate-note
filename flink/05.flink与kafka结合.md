## 安装 Zookeeper

## 安装 Kafka

## 启动 Kafka

1. 启动 Zookeeper

    ```bash
    zkServer.sh start
    ```

1. 启动 Kafka Daemon

    ```bash
    kafka-server-start.sh -daemon /root/apps/kafka_2.11-2.2.0/config/server.properties
    ```

1. 创建 topic

    ```bash
    kafka-topics.sh --create --zookeeper node02:2181 --replication-factor 1 --partitions 1 --topic project_topic
    ```

1. 查看 topic

    ```
    kafka-topics.sh --list --zookeeper node01:2181
    ```

1. 启动 Kafka 生产者

    ```bash
    kafka-console-producer.sh --broker-list node01:9092 --topic pktest
    ```

1. 启动 Kafka 消费者

    ```
    kafka-console-consumer.sh --bootstrap-server node01:9092 --topic pktest
    ```

1. 编写 Kafka 作为 flink Source 的代码：

    ```scala
    import java.util.Properties

    import org.apache.flink.api.common.serialization.SimpleStringSchema
    import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer

    object KafkaConnectorConsumerApp {
        def main(args: Array[String]): Unit = {
            val environment = StreamExecutionEnvironment.getExecutionEnvironment

            import org.apache.flink.api.scala._
            val properties = new Properties()
            properties.setProperty("bootstrap.servers", "192.168.60.21:9092")
            val data = environment.addSource(new FlinkKafkaConsumer[String]("pktest", new SimpleStringSchema(), properties))
            data.print()


            environment.execute("KafkaConnectorConsumerApp")
        }
    }
    ```

    启动 flink 代码，在 Producer 界面输入数据，可以看到 java 终端中收集到的数据。

1. 编写 Kafka 作为 flink Sink 的代码:

    ```scala
    import java.util.Properties

    import org.apache.flink.api.common.serialization.SimpleStringSchema
    import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, FlinkKafkaProducer}
    import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper

    object KafkaConnectorProducerApp {
        def main(args: Array[String]): Unit = {
            val environment = StreamExecutionEnvironment.getExecutionEnvironment

            val data = environment.socketTextStream("localhost", 9999)
            val properties = new Properties()
            properties.setProperty("bootstrap.servers", "node01:9092")
            val kafkaSink = new FlinkKafkaProducer[String]("pktest",
                new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()),
                properties)

            data.addSink(kafkaSink)

            environment.execute("KafkaConnectorProducerApp")
        }
    }
    ```


